{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Basic Neural Networks with LENGTH\n",
    "\n",
    "This is the first practical exercise of our course [Applied Edge AI](https://learn.ki-campus.org/courses/edgeai-hpi2022).\n",
    "In this exercise, you are going to implement a few basic functions and library components of neural networks from scratch in Python.\n",
    "\n",
    "Before we can actually start, install our pip package, which installs the surrounding library LENGTH (Lightning-fast Extensible Neural-network Guarding The HPI), by running the following cell (click the triangular \"Play\" button next to the cell or in the top bar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4b88e7db-9419-493a-b4fb-aaf1a8efd76c",
    "_uuid": "c71811f0-2527-41cc-a68a-3d70412c15cd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install length_hpi\n",
    "# if you get the warning \"Failed to establish a new connection\", go to the side bar on the right, then \"Settings\" and switch on \"Internet\"\n",
    "# you can safely ignore errors such as \"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. [...]\"\n",
    "# since we only need our length package, Pillow, and numpy in this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to prepare a few imports for the following code cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "15444bfe-13a7-4ae5-9d90-3c75b94b18c3",
    "_uuid": "4496c81d-1702-472b-a547-f46cbf085b36",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import length.functions as F\n",
    "\n",
    "import length.tests.optimizer_tests.test_sgd as sgd_tests\n",
    "import length.tests.layer_tests.test_fully_connected as fc_tests\n",
    "import length.tests.function_tests.test_mean_squared_error as mse_tests\n",
    "import length.tests.function_tests.test_relu as relu_tests\n",
    "\n",
    "from length import constants\n",
    "from length.data_sets import MNIST, FashionMNIST\n",
    "from length.function import Function\n",
    "from length.models import MLP\n",
    "from length.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: SGD\n",
    "\n",
    "In one of the lectures in our course we learned about SGD.\n",
    "In the following task we want to compute the parameter delta to actually implement SGD.\n",
    "If you look up the formula on our course slides, the `param_deltas` are subtracted by our framework, thus we do *not* need to multiply our result with -1 in the code.\n",
    "Also note, that the variable `gradients` is the *list* of computed derivatives, thus the derivative part of the formula is already computed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cdd4ef62122aa747",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    An optimizer that does plain Stochastic Gradient Descent\n",
    "    (https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Iterative_method)\n",
    "    :param lr: the learning rate to use for optimization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        self.learning_rate = lr\n",
    "\n",
    "    def run_update_rule(self, gradients, _):\n",
    "        # :param gradients: a list of all computed gradient arrays\n",
    "        # :return: a list of deltas for each provided gradient\n",
    "\n",
    "        # TODO: implement SGD update rule and store the result in a variable called param_deltas (as a list)\n",
    "        # HINT: it can be solved in a single line with a list comprehension ;)\n",
    "        ### BEGIN SOLUTION\n",
    "        #param_deltas = [self.learning_rate * gradient for gradient in gradients]\n",
    "        ### END SOLUTION\n",
    "        return param_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test your solution, you can execute the following code cell.\n",
    "If your solution passes our test, it will simply print `Test passed.`\n",
    "If it does not work, you will get an error.\n",
    "In this case you need to fix the code above.\n",
    "(And do not forget to run the above code cell again!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "point-1"
    ]
   },
   "outputs": [],
   "source": [
    "sgd_tests.SGD = SGD\n",
    "sgd_tests.test_sgd()\n",
    "print(\"Test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Fully Connected Layer\n",
    "\n",
    "It seems we have not learned a lot about a \"fully connected\" layer (sometimes also called a \"Dense\" layer) in our lecture so far.\n",
    "However we learned about Perceptrons and Multi Layer Perceptrons (MLPs), and fully connected layers are the building block of these early models for machine learning.\n",
    "\n",
    "They simply store a weight between each possible input and output value and a bias for each output value.\n",
    "For example, if we have 2 inputs $i_0,i_1$ and 3 outputs $o_0,o_1,o_2$, we store 6 weight values $w_{00}, w_{01}, w_{10}, w_{11}, w_{20},w_{21}$, one value for each pair of one input and one output, and three bias values $b_0,b_1,b_2$, one for each output.\n",
    "Then during the forward pass the outputs of a fully connected layer are calculated as\n",
    "$$o_x = \\sum_{y=0}^1{(i_y \\cdot w_{xy}) + b_x} \\text{ for } 0 \\leq x < 3.$$\n",
    "This is simplified for a single element but in a neural network we work with mini-batches (processing a small number of samples at the same time).\n",
    "In this case, we can use the [dot product](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) of two arrays to simplify this element-wise multiplication.\n",
    "\n",
    "Here is a small very simple code snippet for the forward pass of our example case above with a batch size of 5.\n",
    "The batch axis complicates matters slightly, but pay attention to the shape of each array and how the output size changes.\n",
    "You can play around with different input and output sizes here if you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "num_inputs = 2\n",
    "num_outputs = 3\n",
    "\n",
    "i = np.arange(batch_size * num_inputs).reshape(batch_size, num_inputs)\n",
    "w = np.arange(num_outputs * num_inputs).reshape(num_outputs, num_inputs)\n",
    "b = np.zeros(num_outputs)\n",
    "print(f\"Inputs {i.shape}:\\n\", i)\n",
    "print(f\"Weights {w.shape}:\\n\", w)\n",
    "print(f\"Bias {b.shape}:\\n\", b)\n",
    "\n",
    "output = np.dot(i, w.T) + b\n",
    "print(f\"Output {output.shape}:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting fact here can help you with implementing the backward pass:\n",
    "applying the dot product on two arrays with shapes (5,2) and (2,3) \"removes\" the common axis with size 2 and results in an array of (5,3).\n",
    "You can use this knowledge to figure out which arrays need to be transposed to get the correct shape during the *backward* pass.\n",
    "\n",
    "Furthermore, you can recap how multiplication (between weights and inputs) affects the gradients in our lecture video [Computational Graph](https://learn.ki-campus.org/courses/edgeai-hpi2022/items/3btmrU8Ds8rVDk8SEUz1pU).\n",
    "(Remember instead of using simple multiplication we can use the dot product for arrays.)\n",
    "In the same video you can also recap what happens when we add *two* values in a computational graph (in this case the result of the dot product and our bias), so you can later on implement the *backward* pass for the *bias* correctly.\n",
    "\n",
    "Also you may have noted, that we *transposed* the weight array in the example above - \"swapping\" the axes from (3,2) to (2,3) - with numpy before the dot product by calling `.T`.\n",
    "\n",
    "This is because we recommend storing the weight array in a *transposed* way in your implementation (similar to our example above).\n",
    "\n",
    "One final hint before you are ready to start implementing: we use the variable name `x` for the inputs (`grad_x` for the gradients with respect to the inputs) in our code below, instead of `i` in the example above (we only used `i` above so it maches the previous formula)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5391cae9581cdcc0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from length.layer import Layer\n",
    "from length.constants import DTYPE\n",
    "from length.initializers.xavier import Xavier\n",
    "\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    \"\"\"\n",
    "    The FullyConnected Layer is one of the base building blocks of a neural network. It computes a weighted sum\n",
    "    over the input, using a weight matrix. It furthermore applies a bias term to this weighted sum to allow linear\n",
    "    shifts of the computed values.\n",
    "    \"\"\"\n",
    "    name = \"FullyConnected\"\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, weight_init=Xavier()):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: initialize our weights with correct shape, using the weight initializer 'weight_init'\n",
    "        # here are two hints:\n",
    "        # 1. store an array of zeros in `self._weights` with the correct shape (we recommend storing it transposed as in our simple example above) and use dtype=DTYPE\n",
    "        # 2. call `weight_init` with our freshly created array `self._weights` to initialize the array properly\n",
    "        ### BEGIN SOLUTION\n",
    "        self._weights = np.zeros((num_outputs, num_inputs,), dtype=DTYPE)\n",
    "        weight_init(self._weights)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # TODO: initialize `self.bias` with an array of zeros in the correct shape and use dtype=DTYPE\n",
    "        ### BEGIN SOLUTION\n",
    "        self.bias = np.zeros((num_outputs,), dtype=DTYPE)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        # Transform weights between internal and external representation\n",
    "        return self._weights.T\n",
    "\n",
    "    @weights.setter\n",
    "    def weights(self, value):\n",
    "        # Transform weights between internal and external representation\n",
    "        self._weights = value.T\n",
    "\n",
    "    def internal_forward(self, inputs):\n",
    "        x, = inputs\n",
    "        \n",
    "        # TODO: calculate the output of this layer and store it in a variable `result`\n",
    "        #       (hint: you can look at our simple example above)\n",
    "        ### BEGIN SOLUTION\n",
    "        result = np.dot(x, self._weights.T) + self.bias\n",
    "        ### END SOLUTION\n",
    "        return result,\n",
    "\n",
    "    def internal_backward(self, inputs, gradients):\n",
    "        x, = inputs\n",
    "        grad_in, = gradients\n",
    "        \n",
    "        # TODO: calculate gradients with respect to inputs for this layer\n",
    "        # 1. calculate and store gradients for (the batch of) the inputs `x` in `grad_x`\n",
    "        #    (hint: instead of simple multiplication we need to use the dot product for arrays)\n",
    "        # 2. calculate and store gradients for the weights `w` in `grad_w`\n",
    "        #    (hint: the shapes of `grad_w` and `self._weights` must be equal, so try to figure out which axes is \"removed\" by applying the dot product)\n",
    "        # 3. calculate and store gradients for the bias `b` in `grad_b`\n",
    "        #    (hint: gradients from multiple sources in the computational graph need to be added up)\n",
    "        ### BEGIN SOLUTION\n",
    "        grad_x = np.dot(grad_in, self._weights)\n",
    "        grad_w = np.dot(grad_in.T, x)\n",
    "        grad_b = np.sum(grad_in, axis=0)\n",
    "        ### END SOLUTION\n",
    "\n",
    "        assert grad_x.shape == x.shape\n",
    "        assert grad_w.shape == self._weights.shape\n",
    "        assert grad_b.shape == self.bias.shape\n",
    "\n",
    "        return grad_x, grad_w, grad_b\n",
    "\n",
    "    def internal_update(self, parameter_deltas):\n",
    "        delta_w, delta_b = parameter_deltas\n",
    "        \n",
    "        # TODO: apply updates to weights (self._weights) and bias (self.bias) according to deltas from optimizer\n",
    "        # if you remember our instructions on how to implement SGD, we said: \"[...] the param_deltas are subtracted by our framework [...]\"\n",
    "        # so this is all we need to do here.\n",
    "        ### BEGIN SOLUTION\n",
    "        self._weights -= delta_w\n",
    "        self.bias -= delta_b\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test your solution, you can execute the following code cell.\n",
    "If your solution passes our test, it will simply print `Test passed.`\n",
    "If it does not work, you will get an error.\n",
    "In this case you need to fix the code above.\n",
    "(And do not forget to run the above code cell again!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "point-2"
    ]
   },
   "outputs": [],
   "source": [
    "fc_tests.FullyConnected = FullyConnected\n",
    "fc_tests.test_initialization()\n",
    "fc_tests.test_fully_connected_forward()\n",
    "fc_tests.test_fully_connected_backward()\n",
    "print(\"Test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Mean Squared Error\n",
    "\n",
    "To train a model, we also need a loss function.\n",
    "These loss functions mathematically define how our model should be optimized (and thus learn to solve a certain task).\n",
    "\n",
    "A very simple loss function, which still can be effective in training models is the [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error).\n",
    "Therefore in the following task, we are going to implement this function.\n",
    "The corresponding [wikipedia article](https://en.wikipedia.org/wiki/Mean_squared_error) should explain everything you need to know, if you are not yet familiar with the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6fe26c809f0b5386",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanSquaredError(Function):\n",
    "    \"\"\"\n",
    "    This function calculates the Mean Squared Error between two given vectors, as described in\n",
    "    https://en.wikipedia.org/wiki/Mean_squared_error\n",
    "    \"\"\"\n",
    "    name = \"MeanSquaredError\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: add more initialization if necessary\n",
    "        ### BEGIN SOLUTION\n",
    "        self.difference = None\n",
    "        ### END SOLUTION\n",
    "\n",
    "    @staticmethod\n",
    "    def create_one_hot(data, shape):\n",
    "        assert len(shape) == 2, \"Providing integers as second input to MSE only works with two dimensional input vectors\"\n",
    "        # TODO: create a one-hot representation out of the given label vector (with dtype=DTYPE)\n",
    "        # Example: assume `data` is [2, 3, 0], and the desired `shape` is (3, 4)\n",
    "        #          in this case we have 4 possible classes and 3 samples, belonging to class 2, class 3, and class 0 \n",
    "        #          therefore we need to set a 1 at position 2, 3, 0 for each sample respectively\n",
    "        #          the resulting vector should look like this:\n",
    "        #          result = [[0, 0, 1, 0], [0, 0, 0, 1], [1, 0, 0, 0]]\n",
    "        # Hint: initialize an array of zeros with the given `shape`, set 1s where needed and in the end return your created array\n",
    "        ### BEGIN SOLUTION\n",
    "        data_container = np.zeros(shape, dtype=DTYPE)\n",
    "        data_container[np.arange(len(data)), data] = 1        \n",
    "        return data_container\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def internal_forward(self, inputs):\n",
    "        x1, x2 = inputs\n",
    "\n",
    "        if np.issubdtype(x2.dtype, np.integer):\n",
    "            x2 = self.create_one_hot(x2, x1.shape)\n",
    "\n",
    "        # TODO: calculate the mean squared error of x1 and x2 and store the result in `mean_squared_error`\n",
    "        # hint: you can store an intermediate result as a class member variable here that you need during the backward pass\n",
    "        ### BEGIN SOLUTION\n",
    "        #self.difference = x1 - x2\n",
    "        squared_sum = np.sum(np.square(self.difference))\n",
    "        mean_squared_error = (squared_sum / self.difference.size)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        return mean_squared_error.astype(DTYPE),\n",
    "\n",
    "    def internal_backward(self, inputs, gradients):\n",
    "        x1, x2 = inputs\n",
    "        gx, = gradients\n",
    "        \n",
    "        # TODO: calculate the gradients of this function with respect to its (two) inputs\n",
    "        # (hint: the derivative depends on the inputs (here you could use an intermediate \n",
    "        #        result from the forward pass) and the size of the inputs)\n",
    "        ### BEGIN SOLUTION\n",
    "        derived_value = 2 / x1.size * self.difference\n",
    "        gradient_1 = derived_value * gx\n",
    "        gradient_2 = -gradient_1\n",
    "        ### END SOLUTION\n",
    "\n",
    "        if np.issubdtype(x2.dtype, np.integer):\n",
    "            # in case we used MSE as loss function, we won't propagate any gradients to the loss\n",
    "            return gradient_1, None\n",
    "\n",
    "        return gradient_1, gradient_2\n",
    "\n",
    "\n",
    "def mean_squared_error(input_1, input_2):\n",
    "    \"\"\"\n",
    "    This function calculates the Mean Squared Error between input_1 and input_2. Both inputs should be vectors of the\n",
    "    same shape. You can also supply a one-dimensional list of integers.\n",
    "    If you do so this vector will be converted to a one_hot representation that fits to the shape of the second\n",
    "    input\n",
    "    :param input_1: the first vector of any shape\n",
    "    :param input_2: the second vector. Needs to have the same shape as the first vector, or be a one-dimensional int vector\n",
    "    :return: the mean squared error between input_1 and input_2\n",
    "    \"\"\"\n",
    "    return MeanSquaredError()(input_1, input_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test your solution, you can execute the following code cell.\n",
    "If your solution passes our test, it will simply print `Test passed.`\n",
    "If it does not work, you will get an error.\n",
    "In this case you need to fix the code above.\n",
    "(And do not forget to run the above code cell again!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "point-3"
    ]
   },
   "outputs": [],
   "source": [
    "mse_tests.MeanSquaredError = MeanSquaredError\n",
    "mse_tests.mean_squared_error = mean_squared_error\n",
    "mse_tests.test_mean_squared_error_forward_zero_loss()\n",
    "mse_tests.test_mean_squared_error_forward_loss()\n",
    "mse_tests.test_mean_squared_error_forward_int_input()\n",
    "mse_tests.test_mean_squared_error_backward()\n",
    "mse_tests.test_mean_squared_error_backward_with_label()\n",
    "print(\"Test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: ReLU (Rectified Linear Unit)\n",
    "\n",
    "In our course we learned about how the ReLU function helped to solve the problem of vanishing gradients in the video [A Concise History of Neural Networks (3/4)](https://learn.ki-campus.org/courses/edgeai-hpi2022/items/22nlBMim7pwAX8A7gTI7qn).\n",
    "\n",
    "In the next task, we are going to implement this function by filling in the missing forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-706ebecfe1eb1671",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Relu(Function):\n",
    "    \"\"\"\n",
    "    The Relu Layer is a non-linear activation\n",
    "    \"\"\"\n",
    "    name = \"ReLU\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: add more initialization of class member variables if necessary\n",
    "        ### BEGIN SOLUTION\n",
    "        self.output = None\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def internal_forward(self, inputs):\n",
    "        x, = inputs\n",
    "        # TODO: calculate forward pass of ReLU function and store it in `activated_inputs`\n",
    "        # (we can store any variables we need for the calculation of the backward pass in a class member variable here as well)\n",
    "        ### BEGIN SOLUTION\n",
    "        activated_inputs = np.maximum(x, np.zeros_like(x))\n",
    "        self.output = activated_inputs\n",
    "        ### END SOLUTION\n",
    "        return activated_inputs,\n",
    "\n",
    "    def internal_backward(self, inputs, gradients):\n",
    "        x, = inputs\n",
    "        grad_in, = gradients\n",
    "        \n",
    "        # TODO: calculate gradients of ReLU function with respect to the input and store it in grad_x\n",
    "        # you can first calculate the derivative of ReLU itself (hint: it depends on the input of the forward pass)\n",
    "        # and then use element-wise multiplication of the calculated derivative with the `grad_in` gradients\n",
    "        ### BEGIN SOLUTION\n",
    "        copy = self.output.copy()\n",
    "        copy[copy > 0] = 1\n",
    "        grad_x = np.multiply(grad_in, copy)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        assert grad_x.shape == x.shape\n",
    "        return grad_x,\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    This function computes the element-wise ReLU activation function (https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
    "    on a given input vector x.\n",
    "    :param x: the input vector\n",
    "    :return: a rectified version of the input vector\n",
    "    \"\"\"\n",
    "    return Relu()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test your solution, you can execute the following code cell.\n",
    "If your solution passes our test, it will simply print `Test passed.`\n",
    "If it does not work, you will get an error.\n",
    "In this case you need to fix the code above.\n",
    "(And do not forget to run the above code cell again!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "point-4"
    ]
   },
   "outputs": [],
   "source": [
    "relu_tests.relu = relu\n",
    "relu_tests.Relu = Relu\n",
    "relu_tests.test_relu_forward()\n",
    "relu_tests.test_relu_backward()\n",
    "print(\"Test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your score!\n",
    "\n",
    "## Please follow these instructions before your run the below code cell.\n",
    "\n",
    "- **Test case** code cells must not be changed or modified.\n",
    "- You can run your tasks in any order but **ONLY** the below code cell should be executed at last. \n",
    "- Make sure you save (Ctrl + S) your notebook at regular intervals to keep your implementation(s) saved.\n",
    "- Uncomment the below code to see results on this notebook.\n",
    "\n",
    "**Note: If you see \"Test passed.\" for all the tasks you get 4.0 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Uncomment & run the below code\n",
    "\n",
    "#%run -m run\n",
    "%run -i test_code_white.py \"./ssnn_with_autograding_script.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
